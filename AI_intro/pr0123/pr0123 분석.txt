SGD를 보면 훈련데이터를 따라서 그래프가 진행되다가 어느 순간부터 정확도가 줄고 loss값의 변화가 거의 생기지 않는다. 
그 이유는 과적합 때문에 생기는 현상으로 분석할 수 있다.

SGD에 momentum을 부여한 결과 훈련 결과의 안정도가 늘어난 것을 할수 있고 SGD만을 사용했을 때 보다 학습 속도가 
빠른 것을 확인할 수 있었다. 그리고 훈련, 검증의 안정도 차이가 많이 나온 걸로 보아 과적합현상이 생겼다는 것을 확인할 수 있다. momnetum을 사용하면 이전의 기울기의 방향과 크기를 누적해서 저장하여 학습을 빠르고 안정하게 한다.

Adam은 SGD에 momentum이 내장되어있는 최적화기로 SGD와 비슷한 양상의 그래프를 그리는 것을 확인 할 수 있었다.
Adam도 SGD+momentum과 비슷하게 학습을 빠르게 진행하였고 정확도 그래프 모양이 안정하게 나왔다. 
Adam : 기울기의 1차 모멘트(방향)와 2차 모멘트(크기)를 모두 추적하여, 파라미터별로 학습률을 자동 조절함으로써
빠르고 안정적인 학습을 수행

3가지의 그래프를 보면 정확도 곡선이 어느정도 학습을 하면 일정하게 유지되는 것을 볼 수 있고 SGD보다 안정적인 
부분에 진입하는게 빠른 것을 확인 할 수 있다. 그리고 3개의 그래프 전부 과적합의 문제를 띄고 있다는 것을 확인할 수 있다. 